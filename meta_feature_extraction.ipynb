{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqEtIh2YQKH6",
        "outputId": "d4dc4623-48ce-4dfd-db20-7a2ab3608961"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting textstat\n",
            "  Downloading textstat-0.7.3-py3-none-any.whl (105 kB)\n",
            "\u001b[K     |████████████████████████████████| 105 kB 7.2 MB/s \n",
            "\u001b[?25hCollecting pyphen\n",
            "  Downloading pyphen-0.13.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 44.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyphen, textstat\n",
            "Successfully installed pyphen-0.13.0 textstat-0.7.3\n"
          ]
        }
      ],
      "source": [
        "!pip install textstat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HM7YGGQxI6fk"
      },
      "outputs": [],
      "source": [
        "# Code to read csv file into Colaboratory:\n",
        "# !pip install -U -q PyDrive #<-run once\n",
        "# !pip install -U -q textstat #<-run once\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7cLXObBJTXa",
        "outputId": "26daa568-95d7-4693-f541-5e4bb4b40f8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "RANDOM_STATE = 42\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import textstat\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "from itertools import chain\n",
        "import scipy as sp\n",
        "import numpy as np\n",
        "import time as tm\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "nltk.download('averaged_perceptron_tagger') #<- run once\n",
        "# nltk.download('omw-1.4') #<- run once\n",
        "nltk.download('wordnet') #<- run once\n",
        "nltk.download('punkt') #<- run once\n",
        "# nltk.download('stopwords') #<- run once"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7qFXv8A5JqpG"
      },
      "outputs": [],
      "source": [
        "# full\n",
        "# files = [\n",
        "#     ['1I_ij6JC8WbdP1Mcu4qhlj1m4VHIBAmVW','news.csv'],\n",
        "#     ['13FxAOQIq28F0RrXSPgFfNEO-wlutAGaT', 'covid.csv'],\n",
        "#     ['1LIXxaFe_l7o6xB5DEX387i7txHuKewT2', 'email.csv'],\n",
        "#     ['1--9yAKlKZvqm-FB6M6Fwjgia-9fmgiGt', 'imdb.csv'],\n",
        "#     ['1-0XbBZZQS6i6Iiwj2IxaIu-nM6r_BlFL', 'review.csv'],\n",
        "#     ['1-2e5XJa2MkG98S03WVynPjszFKMpH1Xd', 'twitter.csv'],\n",
        "#     ['1-48SJqcjlptWsxQkpLhts4J4qWBlQ8Q3', 'reddit.csv'],\n",
        "#     ['1-03qp_JbkztYZFY2CXLa63a5OW4QNI6W', 'finance.csv'],\n",
        "#     ['1--Mm5WfgxopRiWpHAZYgUD58eyNWneUi', 'paper.csv']\n",
        "#     ]\n",
        "\n",
        "files = [\n",
        "    ['1vKAwkMHB44mUGIJl66lbhKqFKJ3gkdJA','news.csv'],\n",
        "    ['1-A1oOQDNjkWFhqyAQp2QrWZRNvv8qgI1', 'covid.csv'],\n",
        "    ['1-3vM3D8ERP1Vf1MSL6azPkAC9c3bQ_6H', 'email.csv'],\n",
        "    ['1-3Ex8eXMs4dQq4ecGDlZ63J3QouPILkd', 'imdb.csv'],\n",
        "    ['1-3ALcG0jlqkF_cDexp__OMRLPx2NKlpk', 'review.csv'],\n",
        "    ['1--rSOEtY4bW_p69lhKjP5egurcKQK7-z', 'twitter.csv'],\n",
        "    ['135gADn2a7vVd7-5kv8gfQIcwIStSdwx1', 'reddit.csv'],\n",
        "    ['1xQgwRxzQscKyW9XMbeKA8LQbon6H7uZZ', 'finance.csv'],\n",
        "    ['1ZJyXiZpyb0Lt66_Q-pVop4EuA_272XqZ', 'paper.csv']\n",
        "    ]\n",
        "\n",
        "for fl in files:\n",
        "  downloaded = drive.CreateFile({'id':fl[0]}) \n",
        "  downloaded.GetContentFile(fl[1])\n",
        "  tm.sleep(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvSW1Jv7Mi3R"
      },
      "source": [
        "## Meta Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8ooQYgihLwf4"
      },
      "outputs": [],
      "source": [
        "# news_df = pd.read_csv(\"news.csv\").dropna()\n",
        "# covid_df = pd.read_csv(\"covid.csv\").dropna()\n",
        "# email_df = pd.read_csv(\"email.csv\").dropna()\n",
        "# imdb_df = pd.read_csv(\"imdb.csv\").dropna()\n",
        "# review_df = pd.read_csv(\"review.csv\").dropna()\n",
        "# twitter_df = pd.read_csv(\"twitter.csv\").dropna()\n",
        "# reddit_df = pd.read_csv(\"reddit.csv\").dropna()\n",
        "# finance_df = pd.read_csv(\"finance.csv\").dropna()\n",
        "# paper_df = pd.read_csv(\"paper.csv\").dropna()\n",
        "\n",
        "news_df = pd.read_csv(\"news.csv\").dropna()[:1000]\n",
        "covid_df = pd.read_csv(\"covid.csv\").dropna()[:1000]\n",
        "email_df = pd.read_csv(\"email.csv\").dropna()[:1000]\n",
        "imdb_df = pd.read_csv(\"imdb.csv\").dropna()[:1000]\n",
        "review_df = pd.read_csv(\"review.csv\").dropna()[:1000]\n",
        "twitter_df = pd.read_csv(\"twitter.csv\").dropna()[:1000]\n",
        "reddit_df = pd.read_csv(\"reddit.csv\").dropna()[:1000]\n",
        "finance_df = pd.read_csv(\"finance.csv\").dropna()[:1000]\n",
        "paper_df = pd.read_csv(\"paper.csv\").dropna()[:1000]\n",
        "\n",
        "\n",
        "dfs = [news_df, covid_df, email_df, imdb_df, review_df, twitter_df, reddit_df, finance_df, paper_df]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFh0YpD0M_np"
      },
      "source": [
        "### General MF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "s95YyJeYO7Bm"
      },
      "outputs": [],
      "source": [
        "dataset = [x[1].split(\".\")[0] for x in files]\n",
        "num_of_docs = [len(d.index) for d in dfs]\n",
        "num_of_sent = [len(d[\"sentiment\"].unique()) for d in dfs]\n",
        "\n",
        "general_df = pd.DataFrame(list(zip(dataset, num_of_docs, num_of_sent)),\n",
        "               columns =['dataset', 'num_of_docs', 'num_of_sent'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIxV4w9YT-Z1"
      },
      "source": [
        "### Corpus Hardness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5EIT39hRSIOp"
      },
      "outputs": [],
      "source": [
        "def svb_fun(dfx):\n",
        "  C = dfx[\"sentiment\"].unique()\n",
        "  k = len(C)\n",
        "  VCi = []\n",
        "  for i in C:\n",
        "    res = set()\n",
        "    dfx[dfx[\"sentiment\"] == i][\"sw_exclude\"].str.split(\" \").apply(res.update)\n",
        "    VCi.append(len(res))\n",
        "  resx = set()\n",
        "  dfx[\"sw_exclude\"].str.split(\" \").apply(resx.update)\n",
        "  VD = len(resx)\n",
        "  D = len(dfx.index)\n",
        "  svb = math.sqrt(1/k * (sum(np.square((np.array(VCi) - VD)/D))))\n",
        "  \n",
        "  return svb\n",
        "\n",
        "def uvb_fun(dfx):\n",
        "  D = len(dfx.index)\n",
        "  n = D\n",
        "  Vdi = np.array(dfx[\"sw_exclude\"].str.split(\" \").apply(lambda x: len(set(x))))\n",
        "  \n",
        "  resx = set()\n",
        "  dfx[\"sw_exclude\"].str.split(\" \").apply(resx.update)\n",
        "  VD = len(resx)\n",
        "\n",
        "  uvb = math.sqrt(1/n * (sum(np.square((np.array(Vdi) - VD)/D))))\n",
        "  \n",
        "  return uvb\n",
        "\n",
        "def sem_fun(dfx):\n",
        "  vct = CountVectorizer()\n",
        "  tt = [dfx[\"sw_exclude\"].str.cat(sep=\" \")]\n",
        "  tx = vct.fit_transform(tt)\n",
        "\n",
        "  tf = np.array(tx.toarray()[0])\n",
        "  PtiD = tf/sum(tf)\n",
        "\n",
        "  i = len(tf)\n",
        "  resx = set()\n",
        "  dfx[\"sw_exclude\"].str.split(\" \").apply(resx.update)\n",
        "  VD = len(resx)\n",
        "\n",
        "  QtiD = (1/i)/sum(1/np.arange(1,VD+1))\n",
        "\n",
        "  sem = sum(PtiD * np.log(PtiD/QtiD))\n",
        "  return sem\n",
        "\n",
        "def short_fun(dfx):\n",
        "  n = len(dfx.index)\n",
        "  di = np.array(dfx['sw_exclude'].str.split(\" \").apply(len))\n",
        "  DL = (1/n) * sum(di)\n",
        "\n",
        "  Vdi = np.array(dfx[\"sw_exclude\"].str.split(\" \").apply(lambda x: len(set(x))))  \n",
        "  VL = (1/n) * sum(Vdi)\n",
        "\n",
        "  VDR = np.log(VL)/np.log(DL)\n",
        "\n",
        "  # average word length\n",
        "  AWL = np.mean([len(i.strip()) for i in dfx[\"sw_exclude\"].str.cat(sep=\" \").split(\" \")])  \n",
        "\n",
        "  return {'DL': DL, 'VL': VL, 'VDR': VDR, 'AWL': AWL}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "T8BCgxleLcrO"
      },
      "outputs": [],
      "source": [
        "sht = [short_fun(i) for i in dfs]\n",
        "hardness_df = pd.DataFrame(list(zip(\n",
        "    dataset, \n",
        "    [svb_fun(i) for i in dfs], \n",
        "    [uvb_fun(i) for i in dfs],\n",
        "    [sem_fun(i) for i in dfs],\n",
        "    [i['VL'] for i in sht],\n",
        "    [i['VDR'] for i in sht],\n",
        "    [i['AWL'] for i in sht]\n",
        "    )),\n",
        "    columns =['dataset',\n",
        "              'svb',\n",
        "              'uvb',\n",
        "              'sem',\n",
        "              'vl',\n",
        "              'vdr',\n",
        "              'awl'\n",
        "    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dBT_gECQg9B"
      },
      "source": [
        "### Corpus Readability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8huQr5WvQkIz"
      },
      "outputs": [],
      "source": [
        "def rdblty(dfx):\n",
        "  tt = dfx['sw_exclude'].str.cat(sep='. ')\n",
        "  fre = textstat.flesch_reading_ease(tt)\n",
        "  smog = textstat.smog_index(tt)\n",
        "  fkg = textstat.flesch_kincaid_grade(tt)\n",
        "  cli = textstat.coleman_liau_index(tt)\n",
        "  ari = textstat.automated_readability_index(tt)\n",
        "  dcr = textstat.dale_chall_readability_score(tt)\n",
        "  lw = textstat.linsear_write_formula(tt)\n",
        "  gf = textstat.gunning_fog(tt)\n",
        "\n",
        "  return [fre, smog, fkg, cli, ari, dcr, lw, gf]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "B0rhJ9dddg__"
      },
      "outputs": [],
      "source": [
        "data1 = pd.DataFrame(dataset, columns=['dataset'])\n",
        "data2 = pd.DataFrame([rdblty(i) for i in dfs], columns=['flesch_reading_ease', \n",
        "                                                    'smog', \n",
        "                                                    'flesch_kincaid_grade', \n",
        "                                                    'coleman_liau_index', \n",
        "                                                    'auto_readability_index', \n",
        "                                                    'dale_chall_readability',\n",
        "                                                    'linsear_write',\n",
        "                                                    'gunning_fog'])\n",
        "rdblty_df = pd.concat([data1,data2], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaELO6P81cy3"
      },
      "source": [
        "### Statistical Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6ZhyxRVauAtc"
      },
      "outputs": [],
      "source": [
        "def word_dist(dfx):\n",
        "  tt = dfx['sw_exclude'].str.split(\" \").apply(len)\n",
        "  min_ = min(tt)\n",
        "  max_ = max(tt)\n",
        "  mean = np.mean(tt)\n",
        "  std_ = np.std(tt)\n",
        "  skw = sp.stats.skew(tt)\n",
        "  krt = sp.stats.kurtosis(tt)\n",
        "  mean_std_ratio = mean/std_\n",
        "\n",
        "  val_count = np.array(dfx['sentiment'].value_counts())\n",
        "  ent = sp.stats.entropy(val_count/sum(val_count))\n",
        "  return [min_, max_, mean, std_, skw, krt, mean_std_ratio, ent]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Sk4YWT8AuBdv"
      },
      "outputs": [],
      "source": [
        "data3 = pd.DataFrame([word_dist(i) for i in dfs], columns=['min', \n",
        "                                                    'max', \n",
        "                                                    'mean', \n",
        "                                                    'std', \n",
        "                                                    'skew', \n",
        "                                                    'kurtosis',\n",
        "                                                    'mean_std_ratio',\n",
        "                                                    'entropy'])\n",
        "word_dist_df = pd.concat([data1,data3], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AS1FWZYTwYkZ"
      },
      "source": [
        "### Lexical Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ezbTnftc9RIe"
      },
      "outputs": [],
      "source": [
        "def lx(l):\n",
        "  return nltk.pos_tag(nltk.word_tokenize(l))\n",
        "\n",
        "def lex(dfx):\n",
        "  tt = dfx['sw_include'].str.lower()\n",
        "  lxx = np.array(list(map(lx, tt)))\n",
        "  pos = [j[1] for j in list(chain(*[i for i in lxx]))]\n",
        "\n",
        "  pos_mean = pd.Series(pos).value_counts()\n",
        "  pos_tuple = list(zip(pos_mean.index,pos_mean))\n",
        "\n",
        "  adj = 0\n",
        "  adp = 0\n",
        "  adv = 0\n",
        "  conj = 0\n",
        "  det = 0\n",
        "  nn = 0\n",
        "  num = 0\n",
        "  part = 0\n",
        "  pron = 0\n",
        "  vrb = 0\n",
        "  oth =  0\n",
        "\n",
        "  for tup in pos_tuple:\n",
        "    ttx = tup[0]\n",
        "    if ttx.startswith('JJ'):\n",
        "      adj+=tup[1]\n",
        "    elif ttx.startswith('RB'):\n",
        "      adv+=tup[1]\n",
        "    elif ttx.startswith('NN'):\n",
        "      nn+=tup[1]\n",
        "    elif ttx.startswith('PRP'):\n",
        "      pron+=tup[1]\n",
        "    elif ttx.startswith('VB'):\n",
        "      vrb+=tup[1]\n",
        "    elif ttx == 'IN':\n",
        "      adp+=tup[1]\n",
        "    elif ttx == 'CC':\n",
        "      conj+=tup[1]\n",
        "    elif ttx == 'DT':\n",
        "      det+=tup[1]\n",
        "    elif ttx == 'CD':\n",
        "      num+=tup[1]\n",
        "    elif ttx == 'RP':\n",
        "      part+=tup[1]\n",
        "    else:\n",
        "      oth+=tup[1]\n",
        "\n",
        "  pos_fin = np.array([adj,adp,adv,conj,det,nn,num,part,pron,vrb,oth])/len(tt)\n",
        "  \n",
        "  return pos_fin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzhjqRRe-lGU",
        "outputId": "125e69cc-f764-4c11-b089-db35a6987a47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "data4 = pd.DataFrame([lex(i) for i in dfs], columns=['adjective', \n",
        "                                                    'adposition', \n",
        "                                                    'adverb', \n",
        "                                                    'conjunction', \n",
        "                                                    'determiner', \n",
        "                                                    'noun',\n",
        "                                                    'numeral',\n",
        "                                                    'particle',\n",
        "                                                    'pronoun',\n",
        "                                                    'verb',\n",
        "                                                    'other'])\n",
        "lex_df = pd.concat([data1,data4], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zP83LDigeiUW"
      },
      "source": [
        "### To CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "DfZxaoQi8nfy"
      },
      "outputs": [],
      "source": [
        "general_df.to_csv(wd+\"mlr_general.csv\", index=False)\n",
        "hardness_df.to_csv(wd+\"mlr_hardness.csv\", index=False)\n",
        "rdblty_df.to_csv(wd+\"mlr_rdblty.csv\", index=False)\n",
        "word_dist_df.to_csv(wd+\"mlr_word_dist.csv\", index=False)\n",
        "lex_df.to_csv(wd+\"mlr_lex.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "MxovStCHejtK"
      },
      "outputs": [],
      "source": [
        "merged_df = general_df\\\n",
        ".join(hardness_df.set_index('dataset'), on='dataset', how='left')\\\n",
        ".join(rdblty_df.set_index('dataset'), on='dataset', how='left')\\\n",
        ".join(word_dist_df.set_index('dataset'), on='dataset', how='left')\\\n",
        ".join(lex_df.set_index('dataset'), on='dataset', how='left')\n",
        "\n",
        "wd = '/content/drive/MyDrive/UNI/COMPSCI 760/assignments/group project/scripts/output/'\n",
        "merged_df.to_csv(wd+\"meta_learning_full.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}