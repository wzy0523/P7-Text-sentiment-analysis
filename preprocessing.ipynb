{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Google Drive authentification for loading datasets from gdrive "
      ],
      "metadata": {
        "id": "yxv_WCotEoiC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BiuO95hCEa4Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebe13c61-60d3-4999-c3a2-e8d1cc3ba817"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 105 kB 23.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 42.6 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Code to read csv file into Colaboratory:\n",
        "!pip install -U -q PyDrive #<-run once\n",
        "!pip install -U -q textstat #<-run once\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Libraries Import"
      ],
      "metadata": {
        "id": "KjmKfuqVFt8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_STATE = 42\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "import time as tm\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger') #<- run once\n",
        "nltk.download('omw-1.4') #<- run once\n",
        "nltk.download('wordnet') #<- run once\n",
        "nltk.download('punkt') #<- run once\n",
        "nltk.download('stopwords') #<- run once"
      ],
      "metadata": {
        "id": "ONanf-2rFwZ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63753ba1-813f-465f-add7-f67695b97182"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Datasets"
      ],
      "metadata": {
        "id": "OZLrqvGtF5UW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files = [\n",
        "    ['1BAKoF0KapnjuITPWck1I133xNsMJktYe','news.csv'],\n",
        "    ['1LbkNORZPmql02cSvnzFnaeH7hsEiwN5r', 'covid.csv'],\n",
        "    ['1qRFsO3345fRrRgpt7MEmsshSU9YSjtWQ', 'email_tr.csv'],\n",
        "    ['1SEQF7-xtk2MvJQfYTHZULFOa-ln4rj10', 'email.csv'],\n",
        "    ['1zGyIbieitVGolpUq65V4g7PI6dLlQ_wi', 'imdb.csv'],\n",
        "    ['1YII5laqXiUtngbGsBnC4vnQA3GT36kgn', 'review.csv'],\n",
        "    ['1CoCExkzRr9_fof_gxQGQ-GWLt2wvvLgv', 'twitter.csv'],\n",
        "    ['1xTgGmwlQzVICPndTJJEkGSJHHVinwvTH', 'paper.json'],\n",
        "    ['12-OnNaw6Lwbq2wUfGgVu1jqkspeqOZ8V', 'finance.csv'],\n",
        "    ['1MsWhQvQXNilgINKUsL4g330MmUmngayp', 'reddit.csv']\n",
        "    ]\n",
        "for fl in files:\n",
        "  downloaded = drive.CreateFile({'id':fl[0]}) \n",
        "  downloaded.GetContentFile(fl[1])\n",
        "  tm.sleep(1)"
      ],
      "metadata": {
        "id": "Jb7BZnWh8EoS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_txt = pd.read_csv('news.csv', \n",
        "                       header=None, \n",
        "                       encoding_errors='ignore',\n",
        "                       names=['sent', 'text']\n",
        "                       )[['text','sent']]\n",
        "news_txt['sent'] = np.select([(news_txt['sent'] == 'positive'),(news_txt['sent'] == 'negative')], [1,-1], default=0)\n",
        "\n",
        "# covid dataset doesn't have neutral sentiment\n",
        "covid_txt = pd.read_csv('covid.csv')[['Description', 'Sentiment']].rename(columns={'Description': 'text', 'Sentiment': 'sent'})\n",
        "covid_txt['sent'] = covid_txt['sent'].replace(0,-1)\n",
        "\n",
        "\n",
        "email1 = pd.read_csv('email_tr.csv')[['email_body','sentiment']].rename(columns={'email_body': 'text', 'sentiment': 'sent'})\n",
        "email1['sent'] = np.select([(email1['sent'] > 3), (email1['sent'] < 3)], [1,-1], default=0)\n",
        "email2 = pd.read_csv('email.csv', sep=\";\")[['email_body','label']].rename(columns={'email_body': 'text', 'label': 'sent'})\n",
        "email_txt = pd.concat([email1,email2],ignore_index=True)\n",
        "email_txt['text'] = email_txt['text'].replace('\\\\n', '')\n",
        "\n",
        "\n",
        "# imdb dataset doesn't have neutral sentiment\n",
        "imdb_txt = pd.read_csv('imdb.csv').rename(columns={'review': 'text', 'sentiment': 'sent'})#.sample(n=10000, random_state=RANDOM_STATE, ignore_index=True)\n",
        "imdb_txt['text'] = imdb_txt['text'].str.replace('<br />','')\n",
        "imdb_txt['sent'] = np.select([(imdb_txt['sent'] == 'positive'),(imdb_txt['sent'] == 'negative')], [1,-1], default=0)\n",
        "\n",
        "twitter_txt = pd.read_csv('twitter.csv', header=None, encoding_errors='ignore')[[5,0]].rename(columns={5: 'text', 0: 'sent'})#.sample(n=10000, random_state=RANDOM_STATE, ignore_index=True)\n",
        "twitter_txt['sent'] = np.select([(twitter_txt['sent'] == 4),(twitter_txt['sent'] == 2)], [1,-1], default=0)\n",
        "\n",
        "# for clothes review, i use the rating: 1,2=neg, 3=neutral, 4,5=pos\n",
        "review_txt = pd.read_csv('review.csv')[['Review Text', 'Rating']].rename(columns={'Review Text': 'text', 'Rating': 'sent'}).dropna()#.sample(n=10000, random_state=RANDOM_STATE, ignore_index=True)\n",
        "review_txt['sent'] = np.select([(review_txt['sent'] > 3),(review_txt['sent'] < 3)], [1,-1], default=0)\n",
        "\n",
        "reddit_txt = pd.read_csv('reddit.csv').rename(columns={'clean_comment': 'text', 'category': 'sent'}).dropna()#.sample(n=10000, random_state=RANDOM_STATE, ignore_index=True)\n",
        "\n",
        "finance_txt = pd.read_csv('finance.csv').rename(columns={'Sentence': 'text', 'Sentiment': 'sent'}).dropna()\n",
        "finance_txt['sent'] = np.select([(finance_txt['sent'] == 'positive'), (finance_txt['sent'] == 'negative')], [1,-1], default=0)\n",
        "\n",
        "f = open('paper.json')\n",
        "f1 = [i['review'] for i in json.load(f)['paper']]\n",
        "f2 = [i for s in f1 for i in s]\n",
        "f.text = [i['text'] for i in f2 if i['lan'] == 'en']\n",
        "f.sent = [i['evaluation'] for i in f2 if i['lan'] == 'en']\n",
        "paper_txt = pd.DataFrame(data={'text': f.text,'sent': f.sent})\n",
        "paper_txt = paper_txt.astype({'sent':'int'})\n",
        "paper_txt['sent'] = np.select([(paper_txt['sent'] >= 1),(paper_txt['sent'] <= -1)], [1,-1], default=0)"
      ],
      "metadata": {
        "id": "FQ7ZWj3MFqnq"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing\n"
      ],
      "metadata": {
        "id": "Mk3-eSMAVj_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### preprocessing functions"
      ],
      "metadata": {
        "id": "-xZzxDT6HH_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def lemmatization(text_c):\n",
        "  lemmed = []\n",
        "  for x in text_c:\n",
        "    n = []\n",
        "    for y in x:\n",
        "      if y[1].startswith('J'):\n",
        "        t = wordnet.ADJ\n",
        "      elif y[1].startswith('V'):\n",
        "        t = wordnet.VERB\n",
        "      elif y[1].startswith('N'):\n",
        "        t = wordnet.NOUN\n",
        "      elif y[1].startswith('R'):\n",
        "        t =  wordnet.ADV\n",
        "      else:\n",
        "        t = None\n",
        "      n.append([y[0], t])\n",
        "\n",
        "    usent = ''\n",
        "    for z in n:\n",
        "      if z[1] is None:\n",
        "        u = lemmatizer.lemmatize(z[0])\n",
        "      else:\n",
        "        u = lemmatizer.lemmatize(z[0], pos = z[1])\n",
        "      usent = usent + u + ' '\n",
        "    lemmed.append(usent.strip())    \n",
        "  return lemmed\n",
        "\n",
        "def sw_removal(txt):\n",
        "  m = [t for t in txt.split(' ') if t not in stop_words]\n",
        "  fin = \" \".join(m)\n",
        "  return fin\n",
        "\n",
        "def pre_process(txt_df):\n",
        "  \n",
        "  txt_col = txt_df['text']\n",
        "  snt = list(txt_df['sent'])\n",
        "\n",
        "  # lowercasing\n",
        "  lwrd = txt_col.str.lower()\n",
        "\n",
        "  # non-alphanumeric removal\n",
        "  chrnum = lwrd.str.replace('[^0-9a-zA-Z/ ]', ' ')\n",
        "\n",
        "  # other character removals\n",
        "  rp1 = chrnum.str.replace(' +', ' ')\n",
        "  rp2 = rp1.str.replace(' s ', ' ')\n",
        "  rp3 = rp2.str.replace('(?:\\@|https?\\://)\\S+', '')\n",
        "\n",
        "  # tokenization and postagger\n",
        "  tkn = rp2.apply(nltk.word_tokenize)\n",
        "  postag = tkn.apply(nltk.pos_tag)\n",
        "\n",
        "  # lemmatization\n",
        "  lemm = lemmatization(postag)\n",
        "\n",
        "  # stopword removal\n",
        "  swr = [sw_removal(t) for t in lemm]\n",
        "  \n",
        "  return pd.DataFrame(list(zip(list(txt_col), lemm, swr, snt)), columns=['ori_text', 'sw_include', 'sw_exclude', 'sentiment'])\n"
      ],
      "metadata": {
        "id": "zqhRUPAnJQCo"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing"
      ],
      "metadata": {
        "id": "Jj0K5oz6MJoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news = pre_process(news_txt)\n",
        "covid = pre_process(covid_txt)\n",
        "email = pre_process(email_txt)\n",
        "imdb = pre_process(imdb_txt)\n",
        "review = pre_process(review_txt)\n",
        "twitter = pre_process(twitter_txt)\n",
        "reddit = pre_process(reddit_txt)\n",
        "finance = pre_process(finance_txt)\n",
        "paper = pre_process(paper_txt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNulSxS8MIto",
        "outputId": "8dd4b212-5012-4d83-a432-35c493fdad98"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:50: FutureWarning: The default value of regex will change from True to False in a future version.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Export to Files"
      ],
      "metadata": {
        "id": "JxXerNNFpGXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rT5WQvPXEIt",
        "outputId": "7fc713b7-eab4-4d81-e6ee-3ba25003c799"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wd = '/content/drive/MyDrive/UNI/COMPSCI 760/assignments/group project/scripts/output/'\n",
        "\n",
        "# news.to_csv(wd+'news_preprocess.csv', index=False)\n",
        "# covid.to_csv(wd+'covid_preprocess.csv', index=False)\n",
        "# email.to_csv(wd+'email_preprocess.csv', index=False)\n",
        "# imdb.to_csv(wd+'imdb_preprocess.csv', index=False)\n",
        "# review.to_csv(wd+'review_preprocess.csv', index=False)\n",
        "# twitter.to_csv(wd+'twitter_preprocess.csv', index=False)\n",
        "# reddit.to_csv(wd+'reddit_preprocess.csv', index=False)\n",
        "# finance.to_csv(wd+'finance_preprocess.csv', index=False)\n",
        "# paper.to_csv(wd+'paper_preprocess.csv', index=False)\n",
        "\n",
        "news.to_csv(wd+'news_preprocess_full.csv', index=False)\n",
        "covid.to_csv(wd+'covid_preprocess_full.csv', index=False)\n",
        "email.to_csv(wd+'email_preprocess_full.csv', index=False)\n",
        "imdb.to_csv(wd+'imdb_preprocess_full.csv', index=False)\n",
        "review.to_csv(wd+'review_preprocess_full.csv', index=False)\n",
        "twitter.to_csv(wd+'twitter_preprocess_full.csv', index=False)\n",
        "reddit.to_csv(wd+'reddit_preprocess_full.csv', index=False)\n",
        "finance.to_csv(wd+'finance_preprocess_full.csv', index=False)\n",
        "paper.to_csv(wd+'paper_preprocess_full.csv', index=False)"
      ],
      "metadata": {
        "id": "6B84lbm5oorf"
      },
      "execution_count": 77,
      "outputs": []
    }
  ]
}