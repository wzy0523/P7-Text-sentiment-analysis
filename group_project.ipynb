{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "group_project",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wzy0523/P7-Text-sentiment-analysis/blob/sean_001_preprocessing/group_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Google Drive authentification for loading datasets from gdrive "
      ],
      "metadata": {
        "id": "yxv_WCotEoiC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BiuO95hCEa4Z"
      },
      "outputs": [],
      "source": [
        "# Code to read csv file into Colaboratory:\n",
        "# !pip install -U -q PyDrive #<-run once\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Libraries Import"
      ],
      "metadata": {
        "id": "KjmKfuqVFt8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_STATE = 42\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "import time as tm\n",
        "import os\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger') #<- run once\n",
        "nltk.download('omw-1.4') #<- run once\n",
        "nltk.download('wordnet') #<- run once\n",
        "nltk.download('punkt') #<- run once\n",
        "nltk.download('stopwords') #<- run once"
      ],
      "metadata": {
        "id": "ONanf-2rFwZ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55f903fe-b912-4e7b-e17d-ccb2806c626c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Datasets"
      ],
      "metadata": {
        "id": "OZLrqvGtF5UW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files = [\n",
        "    ['1BAKoF0KapnjuITPWck1I133xNsMJktYe','news.csv'],\n",
        "    ['1LbkNORZPmql02cSvnzFnaeH7hsEiwN5r', 'covid.csv'],\n",
        "    ['1qRFsO3345fRrRgpt7MEmsshSU9YSjtWQ', 'email_tr.csv'],\n",
        "    ['1SEQF7-xtk2MvJQfYTHZULFOa-ln4rj10', 'email.csv'],\n",
        "    ['1zGyIbieitVGolpUq65V4g7PI6dLlQ_wi', 'imdb.csv'],\n",
        "    ['1YII5laqXiUtngbGsBnC4vnQA3GT36kgn', 'review.csv'],\n",
        "    ['1CoCExkzRr9_fof_gxQGQ-GWLt2wvvLgv', 'twitter.csv']\n",
        "    ]\n",
        "for fl in files:\n",
        "  downloaded = drive.CreateFile({'id':fl[0]}) \n",
        "  downloaded.GetContentFile(fl[1])\n",
        "  tm.sleep(1)"
      ],
      "metadata": {
        "id": "Jb7BZnWh8EoS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_txt = pd.read_csv('news.csv', \n",
        "                       header=None, \n",
        "                       encoding_errors='ignore',\n",
        "                       names=['sent', 'text']\n",
        "                       )[['text','sent']]\n",
        "news_txt['sent'] = np.select([(news_txt['sent'] == 'positive'),(news_txt['sent'] == 'negative')], [1,-1], default=0)\n",
        "\n",
        "# covid dataset doesn't have neutral sentiment\n",
        "covid_txt = pd.read_csv('covid.csv')[['Description', 'Sentiment']].rename(columns={'Description': 'text', 'Sentiment': 'sent'})\n",
        "covid_txt['sent'] = covid_txt['sent'].replace(0,-1)\n",
        "\n",
        "\n",
        "email1 = pd.read_csv('email_tr.csv')[['email_body','sentiment']].rename(columns={'email_body': 'text', 'sentiment': 'sent'})\n",
        "email1['sent'] = np.select([(email1['sent'] > 3), (email1['sent'] < 3)], [1,-1], default=0)\n",
        "email2 = pd.read_csv('email.csv', sep=\";\")[['email_body','label']].rename(columns={'email_body': 'text', 'label': 'sent'})\n",
        "email_txt = pd.concat([email1,email2],ignore_index=True)\n",
        "email_txt['text'] = email_txt['text'].replace('\\\\n', '')\n",
        "\n",
        "\n",
        "# imdb dataset doesn't have neutral sentiment\n",
        "imdb_txt = pd.read_csv('imdb.csv').rename(columns={'review': 'text', 'sentiment': 'sent'}).sample(n=10000, random_state=RANDOM_STATE, ignore_index=True)\n",
        "imdb_txt['text'] = imdb_txt['text'].str.replace('<br />','')\n",
        "imdb_txt['sent'] = np.select([(imdb_txt['sent'] == 'positive'),(imdb_txt['sent'] == 'negative')], [1,-1], default=0)\n",
        "\n",
        "twitter_txt = pd.read_csv('twitter.csv', header=None, encoding_errors='ignore')[[5,0]].rename(columns={5: 'text', 0: 'sent'}).sample(n=10000, random_state=RANDOM_STATE, ignore_index=True)\n",
        "twitter_txt['sent'] = np.select([(twitter_txt['sent'] == 4),(twitter_txt['sent'] == 2)], [1,-1], default=0)\n",
        "\n",
        "# for clothes review, i use the rating: 1,2=neg, 3=neutral, 4,5=pos\n",
        "review_txt = pd.read_csv('review.csv')[['Review Text', 'Rating']].rename(columns={'Review Text': 'text', 'Rating': 'sent'}).dropna().sample(n=10000, random_state=RANDOM_STATE, ignore_index=True)\n",
        "review_txt['sent'] = np.select([(review_txt['sent'] > 3),(review_txt['sent'] < 3)], [1,-1], default=0)"
      ],
      "metadata": {
        "id": "FQ7ZWj3MFqnq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing\n"
      ],
      "metadata": {
        "id": "Mk3-eSMAVj_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### preprocessing functions"
      ],
      "metadata": {
        "id": "-xZzxDT6HH_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def lemmatization(text_c):\n",
        "  lemmed = []\n",
        "  for x in text_c:\n",
        "    n = []\n",
        "    for y in x:\n",
        "      if y[1].startswith('J'):\n",
        "        t = wordnet.ADJ\n",
        "      elif y[1].startswith('V'):\n",
        "        t = wordnet.VERB\n",
        "      elif y[1].startswith('N'):\n",
        "        t = wordnet.NOUN\n",
        "      elif y[1].startswith('R'):\n",
        "        t =  wordnet.ADV\n",
        "      else:\n",
        "        t = None\n",
        "      n.append([y[0], t])\n",
        "\n",
        "    usent = ''\n",
        "    for z in n:\n",
        "      if z[1] is None:\n",
        "        u = lemmatizer.lemmatize(z[0])\n",
        "      else:\n",
        "        u = lemmatizer.lemmatize(z[0], pos = z[1])\n",
        "      usent = usent + u + ' '\n",
        "    lemmed.append(usent.strip())    \n",
        "  return lemmed\n",
        "\n",
        "def sw_removal(txt):\n",
        "  m = [t for t in txt.split(' ') if t not in stop_words]\n",
        "  fin = \" \".join(m)\n",
        "  return fin\n",
        "\n",
        "def pre_process(txt_df):\n",
        "  \n",
        "  txt_col = txt_df['text']\n",
        "  snt = list(txt_df['sent'])\n",
        "\n",
        "  # lowercasing\n",
        "  lwrd = txt_col.str.lower()\n",
        "\n",
        "  # non-alphanumeric removal\n",
        "  chrnum = lwrd.str.replace('[^0-9a-zA-Z/ ]', ' ')\n",
        "\n",
        "  # other character removals\n",
        "  rp1 = chrnum.str.replace(' +', ' ')\n",
        "  rp2 = rp1.str.replace(' s ', ' ')\n",
        "  rp3 = rp2.str.replace('(?:\\@|https?\\://)\\S+', '')\n",
        "\n",
        "  # tokenization and postagger\n",
        "  tkn = rp2.apply(nltk.word_tokenize)\n",
        "  postag = tkn.apply(nltk.pos_tag)\n",
        "\n",
        "  # lemmatization\n",
        "  lemm = lemmatization(postag)\n",
        "\n",
        "  # stopword removal\n",
        "  swr = [sw_removal(t) for t in lemm]\n",
        "  \n",
        "  return pd.DataFrame(list(zip(list(txt_col), lemm, swr, snt)), columns=['ori_text', 'sw_include', 'sw_exclude', 'sentiment'])\n"
      ],
      "metadata": {
        "id": "zqhRUPAnJQCo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing"
      ],
      "metadata": {
        "id": "Jj0K5oz6MJoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news = pre_process(news_txt)\n",
        "covid = pre_process(covid_txt)\n",
        "email = pre_process(email_txt)\n",
        "imdb = pre_process(imdb_txt)\n",
        "review = pre_process(review_txt)\n",
        "twitter = pre_process(twitter_txt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNulSxS8MIto",
        "outputId": "ad638d8c-19f8-47ee-b4c9-ab877947166f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:50: FutureWarning: The default value of regex will change from True to False in a future version.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Export to Files"
      ],
      "metadata": {
        "id": "JxXerNNFpGXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rT5WQvPXEIt",
        "outputId": "04374955-3b22-4e9d-b743-b9596df9facf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wd = '/content/drive/MyDrive/UNI/COMPSCI 760/assignments/group project/scripts/output/'\n",
        "\n",
        "news.to_csv(wd+'news_preprocess.csv', index=False)\n",
        "covid.to_csv(wd+'covid_preprocess.csv', index=False)\n",
        "email.to_csv(wd+'email_preprocess.csv', index=False)\n",
        "imdb.to_csv(wd+'imdb_preprocess.csv', index=False)\n",
        "review.to_csv(wd+'review_preprocess.csv', index=False)\n",
        "twitter.to_csv(wd+'twitter_preprocess.csv', index=False)"
      ],
      "metadata": {
        "id": "6B84lbm5oorf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Meta Extraction <u>(not done)</u>\n",
        "\n"
      ],
      "metadata": {
        "id": "K_HCBHtCT_zi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t = [len(m)+1 for m in twitter[0]]\n",
        "pd.Series(l).describe()"
      ],
      "metadata": {
        "id": "ss1AarumT-bP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "e = [len(m)+1 for m in email[0]]\n",
        "pd.Series(e).describe()"
      ],
      "metadata": {
        "id": "NOhDew1ug0CD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = [len(m)+1 for m in news[0]]\n",
        "pd.Series(n).describe()"
      ],
      "metadata": {
        "id": "CqEUxR9HhYOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news.head()"
      ],
      "metadata": {
        "id": "TpTSOy9L42Rv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}