{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "group_project",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wzy0523/P7-Text-sentiment-analysis/blob/sean_001_preprocessing/group_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Google Drive authentification for loading datasets from gdrive "
      ],
      "metadata": {
        "id": "yxv_WCotEoiC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "BiuO95hCEa4Z"
      },
      "outputs": [],
      "source": [
        "# Code to read csv file into Colaboratory:\n",
        "# !pip install -U -q PyDrive #<-run once\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Libraries Import"
      ],
      "metadata": {
        "id": "KjmKfuqVFt8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_STATE = 42\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "import time as tm\n",
        "import os\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger') #<- run once\n",
        "nltk.download('omw-1.4') #<- run once\n",
        "nltk.download('wordnet') #<- run once\n",
        "nltk.download('punkt') #<- run once\n",
        "nltk.download('stopwords') #<- run once"
      ],
      "metadata": {
        "id": "ONanf-2rFwZ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b0304b1-bf1b-47b6-9e4c-53586fc0935a"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Datasets"
      ],
      "metadata": {
        "id": "OZLrqvGtF5UW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files = [\n",
        "    ['1BAKoF0KapnjuITPWck1I133xNsMJktYe','news.csv'],\n",
        "    ['1LbkNORZPmql02cSvnzFnaeH7hsEiwN5r', 'covid.csv'],\n",
        "    ['1qRFsO3345fRrRgpt7MEmsshSU9YSjtWQ', 'email_tr.csv'],\n",
        "    ['1JcjgjLKeZulHVDe2weCaHHV-Lex3vGGh', 'email_ts.csv'],\n",
        "    ['1zGyIbieitVGolpUq65V4g7PI6dLlQ_wi', 'imdb.csv'],\n",
        "    ['1YII5laqXiUtngbGsBnC4vnQA3GT36kgn', 'review.csv'],\n",
        "    ['1CoCExkzRr9_fof_gxQGQ-GWLt2wvvLgv', 'twitter.csv']\n",
        "    ]\n",
        "for fl in files:\n",
        "  downloaded = drive.CreateFile({'id':fl[0]}) \n",
        "  downloaded.GetContentFile(fl[1])\n",
        "  tm.sleep(1)"
      ],
      "metadata": {
        "id": "Jb7BZnWh8EoS"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_txt = pd.read_csv('news.csv', header=None, encoding_errors='ignore')[1]\n",
        "\n",
        "covid_txt = pd.read_csv('covid.csv')['Description']\n",
        "\n",
        "email_txt = pd.concat([pd.read_csv('email_ts.csv')['email_body'], pd.read_csv('email_tr.csv')['email_body']], ignore_index=True).str.replace('\\n', '')\n",
        "\n",
        "imdb_txt = pd.read_csv('imdb.csv')['review'].str.replace('<br />','').sample(n=10000, random_state=RANDOM_STATE, ignore_index=True)\n",
        "\n",
        "review_txt = pd.read_csv('review.csv')['Review Text'].dropna().sample(n=10000, random_state=RANDOM_STATE, ignore_index=True)\n",
        "\n",
        "twitter_txt = pd.read_csv('twitter.csv', header=None, encoding_errors='ignore')[5].str.replace('(?:\\@|https?\\://)\\S+', '').sample(n=10000, random_state=RANDOM_STATE, ignore_index=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQ7ZWj3MFqnq",
        "outputId": "7f1f49f9-26ff-4932-face-58c1233d54e6"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing\n"
      ],
      "metadata": {
        "id": "Mk3-eSMAVj_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### preprocessing functions"
      ],
      "metadata": {
        "id": "-xZzxDT6HH_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def lemmatization(text_c):\n",
        "  lemmed = []\n",
        "  for x in text_c:\n",
        "    n = []\n",
        "    for y in x:\n",
        "      if y[1].startswith('J'):\n",
        "        t = wordnet.ADJ\n",
        "      elif y[1].startswith('V'):\n",
        "        t = wordnet.VERB\n",
        "      elif y[1].startswith('N'):\n",
        "        t = wordnet.NOUN\n",
        "      elif y[1].startswith('R'):\n",
        "        t =  wordnet.ADV\n",
        "      else:\n",
        "        t = None\n",
        "      n.append([y[0], t])\n",
        "\n",
        "    usent = ''\n",
        "    for z in n:\n",
        "      if z[1] is None:\n",
        "        u = lemmatizer.lemmatize(z[0])\n",
        "      else:\n",
        "        u = lemmatizer.lemmatize(z[0], pos = z[1])\n",
        "      usent = usent + u + ' '\n",
        "    lemmed.append(usent.strip())    \n",
        "  return lemmed\n",
        "\n",
        "def sw_removal(txt):\n",
        "  m = [t for t in txt.split(' ') if t not in stop_words]\n",
        "  fin = \" \".join(m)\n",
        "  return fin\n",
        "\n",
        "def pre_process(txt_col):\n",
        "\n",
        "  # lowercasing\n",
        "  lwrd = txt_col.str.lower()\n",
        "\n",
        "  # non-alphanumeric removal\n",
        "  chrnum = lwrd.str.replace('[^0-9a-zA-Z/ ]', ' ')\n",
        "\n",
        "  # other character removals\n",
        "  rp1 = chrnum.str.replace(' +', ' ')\n",
        "  rp2 = rp1.str.replace(' s ', ' ')\n",
        "  rp3 = rp2.str.replace('http\\S+', '')\n",
        "\n",
        "  # tokenization and postagger\n",
        "  tkn = rp2.apply(nltk.word_tokenize)\n",
        "  postag = tkn.apply(nltk.pos_tag)\n",
        "\n",
        "  # lemmatization\n",
        "  lemm = lemmatization(postag)\n",
        "\n",
        "  # stopword removal\n",
        "  swr = [sw_removal(t) for t in lemm]\n",
        "  \n",
        "  return pd.DataFrame(list(zip(lemm, swr)), columns=['sw_include', 'sw_exclude'])\n"
      ],
      "metadata": {
        "id": "zqhRUPAnJQCo"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing"
      ],
      "metadata": {
        "id": "Jj0K5oz6MJoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news = pre_process(news_txt)\n",
        "covid = pre_process(covid_txt)\n",
        "email = pre_process(email_txt)\n",
        "imdb = pre_process(imdb_txt)\n",
        "review = pre_process(review_txt)\n",
        "twitter = pre_process(twitter_txt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNulSxS8MIto",
        "outputId": "32ab0cd3-5248-4352-e9ad-d00db37607f5"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:42: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:47: FutureWarning: The default value of regex will change from True to False in a future version.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Export to Files"
      ],
      "metadata": {
        "id": "JxXerNNFpGXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rT5WQvPXEIt",
        "outputId": "5a9be37b-1a00-486a-efc3-42483beceb72"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wd = '/content/drive/MyDrive/UNI/COMPSCI 760/assignments/group project/scripts/output/'\n",
        "\n",
        "news.to_csv(wd+'news_preprocess.csv', index=False)\n",
        "covid.to_csv(wd+'covid_preprocess.csv', index=False)\n",
        "email.to_csv(wd+'email_preprocess.csv', index=False)\n",
        "imdb.to_csv(wd+'imdb_preprocess.csv', index=False)\n",
        "review.to_csv(wd+'review_preprocess.csv', index=False)\n",
        "twitter.to_csv(wd+'twitter_preprocess.csv', index=False)"
      ],
      "metadata": {
        "id": "6B84lbm5oorf"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Meta Extraction <u>(not done)</u>\n",
        "\n"
      ],
      "metadata": {
        "id": "K_HCBHtCT_zi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t = [len(m)+1 for m in twitter[0]]\n",
        "pd.Series(l).describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ss1AarumT-bP",
        "outputId": "cf99fd79-a48a-4c8c-ac95-f68828b0c4c1"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    10000.000000\n",
              "mean        61.809200\n",
              "std         33.295018\n",
              "min          1.000000\n",
              "25%         34.000000\n",
              "50%         57.000000\n",
              "75%         88.000000\n",
              "max        145.000000\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "e = [len(m)+1 for m in email[0]]\n",
        "pd.Series(e).describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOhDew1ug0CD",
        "outputId": "c4853d2a-90dd-4e6f-e389-2e75e118808b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count     105.00000\n",
              "mean      177.07619\n",
              "std       200.34920\n",
              "min        10.00000\n",
              "25%        60.00000\n",
              "50%        98.00000\n",
              "75%       215.00000\n",
              "max      1040.00000\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = [len(m)+1 for m in news[0]]\n",
        "pd.Series(n).describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqEUxR9HhYOj",
        "outputId": "b5f671ab-c660-45d1-90f1-440191cdce61"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    4846.000000\n",
              "mean      118.734007\n",
              "std        52.423016\n",
              "min         7.000000\n",
              "25%        78.000000\n",
              "50%       111.000000\n",
              "75%       152.000000\n",
              "max       287.000000\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    }
  ]
}