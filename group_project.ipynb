{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "group_project",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wzy0523/P7-Text-sentiment-analysis/blob/sean_001_preprocessing/group_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Google Drive authentification for loading datasets from gdrive "
      ],
      "metadata": {
        "id": "yxv_WCotEoiC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "BiuO95hCEa4Z"
      },
      "outputs": [],
      "source": [
        "# Code to read csv file into Colaboratory:\n",
        "# !pip install -U -q PyDrive #<-run once\n",
        "# !pip install -U -q textstat #<-run once\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Libraries Import"
      ],
      "metadata": {
        "id": "KjmKfuqVFt8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_STATE = 42\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "import time as tm\n",
        "import os\n",
        "import re\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger') #<- run once\n",
        "nltk.download('omw-1.4') #<- run once\n",
        "nltk.download('wordnet') #<- run once\n",
        "nltk.download('punkt') #<- run once\n",
        "nltk.download('stopwords') #<- run once"
      ],
      "metadata": {
        "id": "ONanf-2rFwZ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "639f4add-313e-4101-eb95-caab2d37a2c5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Datasets"
      ],
      "metadata": {
        "id": "OZLrqvGtF5UW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files = [\n",
        "    ['1BAKoF0KapnjuITPWck1I133xNsMJktYe','news.csv'],\n",
        "    ['1LbkNORZPmql02cSvnzFnaeH7hsEiwN5r', 'covid.csv'],\n",
        "    ['1qRFsO3345fRrRgpt7MEmsshSU9YSjtWQ', 'email_tr.csv'],\n",
        "    ['1SEQF7-xtk2MvJQfYTHZULFOa-ln4rj10', 'email.csv'],\n",
        "    ['1zGyIbieitVGolpUq65V4g7PI6dLlQ_wi', 'imdb.csv'],\n",
        "    ['1YII5laqXiUtngbGsBnC4vnQA3GT36kgn', 'review.csv'],\n",
        "    ['1CoCExkzRr9_fof_gxQGQ-GWLt2wvvLgv', 'twitter.csv']\n",
        "    ]\n",
        "for fl in files:\n",
        "  downloaded = drive.CreateFile({'id':fl[0]}) \n",
        "  downloaded.GetContentFile(fl[1])\n",
        "  tm.sleep(1)"
      ],
      "metadata": {
        "id": "Jb7BZnWh8EoS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_txt = pd.read_csv('news.csv', \n",
        "                       header=None, \n",
        "                       encoding_errors='ignore',\n",
        "                       names=['sent', 'text']\n",
        "                       )[['text','sent']]\n",
        "news_txt['sent'] = np.select([(news_txt['sent'] == 'positive'),(news_txt['sent'] == 'negative')], [1,-1], default=0)\n",
        "\n",
        "# covid dataset doesn't have neutral sentiment\n",
        "covid_txt = pd.read_csv('covid.csv')[['Description', 'Sentiment']].rename(columns={'Description': 'text', 'Sentiment': 'sent'})\n",
        "covid_txt['sent'] = covid_txt['sent'].replace(0,-1)\n",
        "\n",
        "\n",
        "email1 = pd.read_csv('email_tr.csv')[['email_body','sentiment']].rename(columns={'email_body': 'text', 'sentiment': 'sent'})\n",
        "email1['sent'] = np.select([(email1['sent'] > 3), (email1['sent'] < 3)], [1,-1], default=0)\n",
        "email2 = pd.read_csv('email.csv', sep=\";\")[['email_body','label']].rename(columns={'email_body': 'text', 'label': 'sent'})\n",
        "email_txt = pd.concat([email1,email2],ignore_index=True)\n",
        "email_txt['text'] = email_txt['text'].replace('\\\\n', '')\n",
        "\n",
        "\n",
        "# imdb dataset doesn't have neutral sentiment\n",
        "imdb_txt = pd.read_csv('imdb.csv').rename(columns={'review': 'text', 'sentiment': 'sent'}).sample(n=10000, random_state=RANDOM_STATE, ignore_index=True)\n",
        "imdb_txt['text'] = imdb_txt['text'].str.replace('<br />','')\n",
        "imdb_txt['sent'] = np.select([(imdb_txt['sent'] == 'positive'),(imdb_txt['sent'] == 'negative')], [1,-1], default=0)\n",
        "\n",
        "twitter_txt = pd.read_csv('twitter.csv', header=None, encoding_errors='ignore')[[5,0]].rename(columns={5: 'text', 0: 'sent'}).sample(n=10000, random_state=RANDOM_STATE, ignore_index=True)\n",
        "twitter_txt['sent'] = np.select([(twitter_txt['sent'] == 4),(twitter_txt['sent'] == 2)], [1,-1], default=0)\n",
        "\n",
        "# for clothes review, i use the rating: 1,2=neg, 3=neutral, 4,5=pos\n",
        "review_txt = pd.read_csv('review.csv')[['Review Text', 'Rating']].rename(columns={'Review Text': 'text', 'Rating': 'sent'}).dropna().sample(n=10000, random_state=RANDOM_STATE, ignore_index=True)\n",
        "review_txt['sent'] = np.select([(review_txt['sent'] > 3),(review_txt['sent'] < 3)], [1,-1], default=0)"
      ],
      "metadata": {
        "id": "FQ7ZWj3MFqnq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing\n"
      ],
      "metadata": {
        "id": "Mk3-eSMAVj_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### preprocessing functions"
      ],
      "metadata": {
        "id": "-xZzxDT6HH_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def lemmatization(text_c):\n",
        "  lemmed = []\n",
        "  for x in text_c:\n",
        "    n = []\n",
        "    for y in x:\n",
        "      if y[1].startswith('J'):\n",
        "        t = wordnet.ADJ\n",
        "      elif y[1].startswith('V'):\n",
        "        t = wordnet.VERB\n",
        "      elif y[1].startswith('N'):\n",
        "        t = wordnet.NOUN\n",
        "      elif y[1].startswith('R'):\n",
        "        t =  wordnet.ADV\n",
        "      else:\n",
        "        t = None\n",
        "      n.append([y[0], t])\n",
        "\n",
        "    usent = ''\n",
        "    for z in n:\n",
        "      if z[1] is None:\n",
        "        u = lemmatizer.lemmatize(z[0])\n",
        "      else:\n",
        "        u = lemmatizer.lemmatize(z[0], pos = z[1])\n",
        "      usent = usent + u + ' '\n",
        "    lemmed.append(usent.strip())    \n",
        "  return lemmed\n",
        "\n",
        "def sw_removal(txt):\n",
        "  m = [t for t in txt.split(' ') if t not in stop_words]\n",
        "  fin = \" \".join(m)\n",
        "  return fin\n",
        "\n",
        "def pre_process(txt_df):\n",
        "  \n",
        "  txt_col = txt_df['text']\n",
        "  snt = list(txt_df['sent'])\n",
        "\n",
        "  # lowercasing\n",
        "  lwrd = txt_col.str.lower()\n",
        "\n",
        "  # non-alphanumeric removal\n",
        "  chrnum = lwrd.str.replace('[^0-9a-zA-Z/ ]', ' ')\n",
        "\n",
        "  # other character removals\n",
        "  rp1 = chrnum.str.replace(' +', ' ')\n",
        "  rp2 = rp1.str.replace(' s ', ' ')\n",
        "  rp3 = rp2.str.replace('(?:\\@|https?\\://)\\S+', '')\n",
        "\n",
        "  # tokenization and postagger\n",
        "  tkn = rp2.apply(nltk.word_tokenize)\n",
        "  postag = tkn.apply(nltk.pos_tag)\n",
        "\n",
        "  # lemmatization\n",
        "  lemm = lemmatization(postag)\n",
        "\n",
        "  # stopword removal\n",
        "  swr = [sw_removal(t) for t in lemm]\n",
        "  \n",
        "  return pd.DataFrame(list(zip(list(txt_col), lemm, swr, snt)), columns=['ori_text', 'sw_include', 'sw_exclude', 'sentiment'])\n"
      ],
      "metadata": {
        "id": "zqhRUPAnJQCo"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing"
      ],
      "metadata": {
        "id": "Jj0K5oz6MJoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news = pre_process(news_txt)\n",
        "covid = pre_process(covid_txt)\n",
        "email = pre_process(email_txt)\n",
        "imdb = pre_process(imdb_txt)\n",
        "review = pre_process(review_txt)\n",
        "twitter = pre_process(twitter_txt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNulSxS8MIto",
        "outputId": "1356a634-d808-4d4f-d163-3f06af7a9cd8"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:50: FutureWarning: The default value of regex will change from True to False in a future version.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Export to Files"
      ],
      "metadata": {
        "id": "JxXerNNFpGXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rT5WQvPXEIt",
        "outputId": "b8813a8c-da87-4ed4-ff27-04113934115c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wd = '/content/drive/MyDrive/UNI/COMPSCI 760/assignments/group project/scripts/output/'\n",
        "\n",
        "news.to_csv(wd+'news_preprocess.csv', index=False)\n",
        "covid.to_csv(wd+'covid_preprocess.csv', index=False)\n",
        "email.to_csv(wd+'email_preprocess.csv', index=False)\n",
        "imdb.to_csv(wd+'imdb_preprocess.csv', index=False)\n",
        "review.to_csv(wd+'review_preprocess.csv', index=False)\n",
        "twitter.to_csv(wd+'twitter_preprocess.csv', index=False)"
      ],
      "metadata": {
        "id": "6B84lbm5oorf"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Meta Extraction"
      ],
      "metadata": {
        "id": "K_HCBHtCT_zi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# docs = [news, covid, email, imdb, review, twitter]\n",
        "\n",
        "# general meta features extraction\n",
        "def general_meta(df):\n",
        "  \n",
        "  # length\n",
        "  lgt = [len(x) + 1 for x in df['sw_include'].str.split(' ')]\n",
        "\n",
        "  # number of alpha-numeric characters\n",
        "  anum_chr = [len(y) for y in [re.findall('[0-9a-zA-Z/ ]', x) for x in df['ori_text']]]\n",
        "\n",
        "  # number of non-alpha-numeric characters\n",
        "  nanum_chr = [len(y) for y in [re.findall('[^0-9a-zA-Z/ ]', x) for x in df['ori_text']]]\n",
        "\n",
        "  # combine\n",
        "  new_df = df.copy()\n",
        "  new_df[['length', 'anum_char', 'nanum_char']] = pd.DataFrame(list(zip(lgt, anum_chr, nanum_chr)))\n",
        "\n",
        "  return new_df\n",
        "\n",
        "def lex_meta(df):\n",
        "  dic = {\n",
        "    'adjective' : [],\n",
        "    'adposition' : [],\n",
        "    'adverb' : [],\n",
        "    'conjunction' : [],\n",
        "    'determiner' : [],\n",
        "    'noun' : [],\n",
        "    'numeral' : [],\n",
        "    'particle' : [],\n",
        "    'pronoun' : [],\n",
        "    'verb' : [],\n",
        "    # 'punctuation' : [],\n",
        "    'other' : []\n",
        "  }\n",
        "\n",
        "  for t in df['ori_text'].str.lower():\n",
        "    adjective = 0\n",
        "    adposition = 0\n",
        "    adverb = 0\n",
        "    conjunction = 0\n",
        "    determiner = 0\n",
        "    noun = 0\n",
        "    numeral = 0\n",
        "    particle = 0\n",
        "    pronoun = 0\n",
        "    verb = 0\n",
        "    # punctuation = 0\n",
        "    other =  0\n",
        "    for tag in nltk.pos_tag(nltk.word_tokenize(t)):\n",
        "      tt = tag[1]\n",
        "      if tt.startswith('JJ'):\n",
        "        adjective+=1\n",
        "      elif tt.startswith('RB'):\n",
        "        adverb+=1\n",
        "      elif tt.startswith('NN'):\n",
        "        noun+=1\n",
        "      elif tt.startswith('PRP'):\n",
        "        pronoun+=1\n",
        "      elif tt.startswith('VB'):\n",
        "        verb+=1\n",
        "      elif tt == 'IN':\n",
        "        adposition+=1\n",
        "      elif tt == 'CC':\n",
        "        conjunction+=1\n",
        "      elif tt == 'DT':\n",
        "        determiner+=1\n",
        "      elif tt == 'CD':\n",
        "        numeral+=1\n",
        "      elif tt == 'RP':\n",
        "        particle+=1\n",
        "      # elif tt == '!':\n",
        "      #   punctuation+=1\n",
        "      else:\n",
        "        other+=1\n",
        "\n",
        "    dic['adjective'].append(adjective)\n",
        "    dic['adposition'].append(adposition)\n",
        "    dic['adverb'].append(adverb)\n",
        "    dic['conjunction'].append(conjunction)\n",
        "    dic['determiner'].append(determiner)\n",
        "    dic['noun'].append(noun)\n",
        "    dic['numeral'].append(numeral)\n",
        "    dic['particle'].append(particle)\n",
        "    dic['pronoun'].append(pronoun)\n",
        "    dic['verb'].append(verb)\n",
        "    # dic['punctuation'].append(punctuation)\n",
        "    dic['other'].append(other)\n",
        "\n",
        "  new_df = pd.concat([df, pd.DataFrame(dic)], axis=1)\n",
        "  return new_df"
      ],
      "metadata": {
        "id": "sYWYvkWC_95w"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_general_df = general_meta(news)\n",
        "covid_general_df = general_meta(covid)\n",
        "email_general_df = general_meta(email)\n",
        "imdb_general_df = general_meta(imdb)\n",
        "review_general_df = general_meta(review)\n",
        "twitter_general_df = general_meta(twitter)"
      ],
      "metadata": {
        "id": "iwlEWeLz_-XG"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_lex_df = lex_meta(news)\n",
        "covid_lex_df = lex_meta(covid)\n",
        "email_lex_df = lex_meta(email)\n",
        "imdb_lex_df = lex_meta(imdb)\n",
        "review_lex_df = lex_meta(review)\n",
        "twitter_lex_df = lex_meta(twitter)"
      ],
      "metadata": {
        "id": "44TxCh3G3xSW"
      },
      "execution_count": 54,
      "outputs": []
    }
  ]
}